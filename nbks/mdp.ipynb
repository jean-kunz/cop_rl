{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc8ba1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAMPLE TRAJECTORIES FROM DAVID SILVER'S STUDENT MARKOV CHAIN\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Starting from: C1\n",
      "======================================================================\n",
      "\n",
      "Trajectory 1: C1 -> C2 -> C3 -> Pub -> C2 -> C3 -> Pub -> C2 -> C3 -> Pub -> C2 -> C3 -> Pass\n",
      "  Ended in terminal state: Pass\n",
      "\n",
      "Trajectory 2: C1 -> FB -> FB -> FB -> FB -> FB -> FB -> C1 -> C2 -> C3 -> Pass\n",
      "  Ended in terminal state: Pass\n",
      "\n",
      "Trajectory 3: C1 -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> C1 -> C2 -> C3 -> Pass\n",
      "  Ended in terminal state: Pass\n",
      "\n",
      "======================================================================\n",
      "Starting from: C2\n",
      "======================================================================\n",
      "\n",
      "Trajectory 1: C2 -> C3 -> Pub -> C2 -> C3 -> Pub -> C2 -> C3 -> Pass\n",
      "  Ended in terminal state: Pass\n",
      "\n",
      "Trajectory 2: C2 -> C3 -> Pub -> C1 -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> C1\n",
      "\n",
      "Trajectory 3: C2 -> C3 -> Pub -> C3 -> Pass\n",
      "  Ended in terminal state: Pass\n",
      "\n",
      "======================================================================\n",
      "Starting from: C3\n",
      "======================================================================\n",
      "\n",
      "Trajectory 1: C3 -> Pub -> C3 -> Pass\n",
      "  Ended in terminal state: Pass\n",
      "\n",
      "Trajectory 2: C3 -> Pass\n",
      "  Ended in terminal state: Pass\n",
      "\n",
      "Trajectory 3: C3 -> Pub -> C2 -> C3 -> Pub -> C2 -> Sleep\n",
      "  Ended in terminal state: Sleep\n",
      "\n",
      "======================================================================\n",
      "Starting from: FB\n",
      "======================================================================\n",
      "\n",
      "Trajectory 1: FB -> FB -> FB -> FB -> C1 -> C2 -> Sleep\n",
      "  Ended in terminal state: Sleep\n",
      "\n",
      "Trajectory 2: FB -> FB -> FB -> FB -> C1 -> C2 -> C3 -> Pub -> C1 -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> FB -> C1 -> FB -> FB\n",
      "\n",
      "Trajectory 3: FB -> C1 -> FB -> FB -> FB -> FB -> C1 -> C2 -> C3 -> Pass\n",
      "  Ended in terminal state: Pass\n",
      "\n",
      "======================================================================\n",
      "Starting from: Pub\n",
      "======================================================================\n",
      "\n",
      "Trajectory 1: Pub -> C2 -> C3 -> Pass\n",
      "  Ended in terminal state: Pass\n",
      "\n",
      "Trajectory 2: Pub -> C1 -> C2 -> Sleep\n",
      "  Ended in terminal state: Sleep\n",
      "\n",
      "Trajectory 3: Pub -> C3 -> Pass\n",
      "  Ended in terminal state: Pass\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# David Silver's Student Markov Chain\n",
    "# States: C1 (Class1), C2 (Class2), C3 (Class3), FB (Facebook), Pub, Pass, Sleep\n",
    "\n",
    "\n",
    "class StudentMarkovChain:\n",
    "    def __init__(self):\n",
    "        self.states = [\"C1\", \"C2\", \"C3\", \"FB\", \"Pub\", \"Pass\", \"Sleep\"]\n",
    "        self.terminal_states = [\"Pass\", \"Sleep\"]\n",
    "\n",
    "        # Transition probabilities: {state: [(prob, next_state), ...]}\n",
    "        # Simplified model where student makes random choices at each state\n",
    "        self.transitions = {\n",
    "            \"C1\": [\n",
    "                (0.5, \"C2\"),  # Study\n",
    "                (0.5, \"FB\"),  # Facebook\n",
    "            ],\n",
    "            \"C2\": [\n",
    "                (0.8, \"C3\"),  # Study\n",
    "                (0.2, \"Sleep\"),  # Sleep\n",
    "            ],\n",
    "            \"C3\": [\n",
    "                (0.6, \"Pass\"),\n",
    "                (0.4, \"Pub\"),\n",
    "            ],\n",
    "            \"FB\": [\n",
    "                (0.9, \"FB\"),  # Keep browsing\n",
    "                (0.1, \"C1\"),  # Quit\n",
    "            ],\n",
    "            \"Pub\": [\n",
    "                (0.2, \"C1\"),  # Leave pub, go to C1 (0.5 * 0.2)\n",
    "                (0.4, \"C2\"),  # Leave pub, go to C2 (0.5 * 0.4)\n",
    "                (0.4, \"C3\"),  # Leave pub, go to C3 (0.5 * 0.4)\n",
    "            ],\n",
    "            \"Pass\": [],  # Terminal state\n",
    "            \"Sleep\": [],  # Terminal state\n",
    "        }\n",
    "\n",
    "    def step(self, state):\n",
    "        \"\"\"Take a step in the Markov Chain from current state\"\"\"\n",
    "        if state in self.terminal_states:\n",
    "            return state\n",
    "\n",
    "        if state not in self.transitions:\n",
    "            raise ValueError(f\"Invalid state: {state}\")\n",
    "\n",
    "        # Get possible transitions\n",
    "        possible_transitions = self.transitions[state]\n",
    "\n",
    "        if not possible_transitions:\n",
    "            return state\n",
    "\n",
    "        # Sample next state based on probabilities\n",
    "        probs = [t[0] for t in possible_transitions]\n",
    "        idx = np.random.choice(len(possible_transitions), p=probs)\n",
    "        _, next_state = possible_transitions[idx]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def generate_trajectory(self, start_state, max_steps=20):\n",
    "        \"\"\"Generate a sample trajectory starting from start_state\"\"\"\n",
    "        trajectory = [start_state]\n",
    "        state = start_state\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            if state in self.terminal_states:\n",
    "                break\n",
    "\n",
    "            next_state = self.step(state)\n",
    "            trajectory.append(next_state)\n",
    "            state = next_state\n",
    "\n",
    "        return trajectory\n",
    "\n",
    "\n",
    "# Create the Markov Chain\n",
    "mc = StudentMarkovChain()\n",
    "\n",
    "# Generate sample trajectories from different starting states\n",
    "print(\"=\" * 70)\n",
    "print(\"SAMPLE TRAJECTORIES FROM DAVID SILVER'S STUDENT MARKOV CHAIN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "starting_states = [\"C1\", \"C2\", \"C3\", \"FB\", \"Pub\"]\n",
    "\n",
    "for start_state in starting_states:\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Starting from: {start_state}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    for i in range(3):  # 3 sample trajectories per starting state\n",
    "        trajectory = mc.generate_trajectory(start_state)\n",
    "\n",
    "        print(f\"\\nTrajectory {i + 1}: {' -> '.join(trajectory)}\")\n",
    "\n",
    "        if trajectory[-1] in mc.terminal_states:\n",
    "            print(f\"  Ended in terminal state: {trajectory[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f381a9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MRP STATE REWARDS r(s)\n",
      "======================================================================\n",
      "  Class1 :  -2.00\n",
      "  Class2 :  -2.00\n",
      "  Class3 :  -2.00\n",
      "Facebook :  -1.00\n",
      "     Pub :   1.00\n",
      "   Sleep :   0.00\n",
      "    Pass :  10.00\n",
      "\n",
      "======================================================================\n",
      "MRP VALUE FUNCTION V(s) (NO ACTIONS)\n",
      "======================================================================\n",
      "  Class1  V=-19.950617\n",
      "  Class2  V= -5.950617\n",
      "  Class3  V= -4.938272\n",
      "Facebook  V=-29.950617\n",
      "     Pub  V= -7.345679\n",
      "   Sleep  V=  0.000000\n",
      "    Pass  V=  0.000000\n"
     ]
    }
   ],
   "source": [
    "# Markov Reward Process (MRP): no actions, just P(s'|s) and r(s).\n",
    "# We'll use the Markov chain dynamics from StudentMarkovChain as the MRP transition model.\n",
    "\n",
    "state_map = {\n",
    "    \"C1\": \"Class1\",\n",
    "    \"C2\": \"Class2\",\n",
    "    \"C3\": \"Class3\",\n",
    "    \"FB\": \"Facebook\",\n",
    "    \"Pub\": \"Pub\",\n",
    "    \"Sleep\": \"Sleep\",\n",
    "    \"Pass\": \"Pass\",\n",
    "}\n",
    "\n",
    "\n",
    "class StudentMRP:\n",
    "    def __init__(self, mc: StudentMarkovChain):\n",
    "        self.states = [\"Class1\", \"Class2\", \"Class3\", \"Facebook\", \"Pub\", \"Sleep\", \"Pass\"]\n",
    "        self.terminal_states = [\"Sleep\", \"Pass\"]\n",
    "\n",
    "        # Transition probabilities P(s'|s) induced by the Markov chain\n",
    "        self.P = {s: [] for s in self.states}\n",
    "        for s_short, trans in mc.transitions.items():\n",
    "            s = state_map[s_short]\n",
    "            self.P[s] = [(p, state_map[s_next]) for p, s_next in trans]\n",
    "\n",
    "        # Optional: treat terminal states as absorbing (not required for evaluation below)\n",
    "        for t in self.terminal_states:\n",
    "            self.P[t] = [(1.0, t)]\n",
    "\n",
    "        # State rewards r(s) (edit these as desired)\n",
    "        self.r = {\n",
    "            \"Class1\": -2.0,\n",
    "            \"Class2\": -2.0,\n",
    "            \"Class3\": -2.0,\n",
    "            \"Facebook\": -1.0,\n",
    "            \"Pub\": 1.0,\n",
    "            \"Sleep\": 0.0,\n",
    "            \"Pass\": 10.0,\n",
    "        }\n",
    "\n",
    "    def transitions(self, state):\n",
    "        return self.P[state]\n",
    "\n",
    "    def reward(self, state):\n",
    "        return float(self.r[state])\n",
    "\n",
    "\n",
    "def mrp_value_function(mrp: StudentMRP, gamma=0.9, theta=1e-12, max_iters=100_000):\n",
    "    \"\"\"Evaluate V(s) for an MRP: V(s) = r(s) + gamma * sum_{s'} P(s'|s) V(s').\"\"\"\n",
    "    V = {s: 0.0 for s in mrp.states}\n",
    "\n",
    "    # Keep terminal values pinned at 0.0 (common convention for episodic tasks)\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in mrp.states:\n",
    "            if s in mrp.terminal_states:\n",
    "                continue\n",
    "            v_old = V[s]\n",
    "            exp_next = sum(p * V[s_next] for p, s_next in mrp.transitions(s))\n",
    "            V[s] = mrp.reward(s) + gamma * exp_next\n",
    "            delta = max(delta, abs(v_old - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "\n",
    "mrp = StudentMRP(mc)\n",
    "V_mrp = mrp_value_function(mrp, gamma=1.0)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MRP STATE REWARDS r(s)\")\n",
    "print(\"=\" * 70)\n",
    "for s in mrp.states:\n",
    "    print(f\"{s:>8s} : {mrp.r[s]:>6.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MRP VALUE FUNCTION V(s) (NO ACTIONS)\")\n",
    "print(\"=\" * 70)\n",
    "for s in mrp.states:\n",
    "    print(f\"{s:>8s}  V={V_mrp[s]:>10.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d055a2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ACTION REWARDS\n",
      "======================================================================\n",
      "     drink :   1.00\n",
      "  facebook :  -1.00\n",
      "       pub :   1.00\n",
      "      quit :   0.00\n",
      "     sleep :   0.00\n",
      "     study :  -2.00\n",
      "\n",
      "======================================================================\n",
      "OPTIMAL VALUE FUNCTION V(s) AND BEST ACTION (pi(s))\n",
      "======================================================================\n",
      "  Class1  V= -1.7640   pi(s)=study\n",
      "  Class2  V=  0.2623   pi(s)=study\n",
      "  Class3  V=  2.5136   pi(s)=pub\n",
      "Facebook  V= -1.5876   pi(s)=quit\n",
      "     Pub  V=  1.6818   pi(s)=drink\n",
      "   Sleep  V=  0.0000   pi(s)=(terminal)\n",
      "    Pass  V=  0.0000   pi(s)=(terminal)\n"
     ]
    }
   ],
   "source": [
    "# Turn the Student Markov Chain into an MDP by making actions explicit.\n",
    "# In an MRP there are no actions (only states + rewards).\n",
    "# NOTE: 'study' is an ACTION, not a state.\n",
    "# States are: Class1, Class2, Class3, Facebook, Pub, Sleep, Pass.\n",
    "\n",
    "\n",
    "class StudentMDP:\n",
    "    def __init__(self):\n",
    "        self.states = [\"Class1\", \"Class2\", \"Class3\", \"Facebook\", \"Pub\", \"Sleep\", \"Pass\"]\n",
    "        self.terminal_states = [\"Sleep\", \"Pass\"]\n",
    "\n",
    "        # Available actions per state\n",
    "        self.actions = {\n",
    "            \"Class1\": [\"study\", \"facebook\"],\n",
    "            \"Class2\": [\"study\", \"sleep\"],\n",
    "            \"Class3\": [\"study\", \"pub\"],\n",
    "            \"Facebook\": [\"facebook\", \"quit\"],\n",
    "            \"Pub\": [\"drink\"],\n",
    "            \"Sleep\": [],\n",
    "            \"Pass\": [],\n",
    "        }\n",
    "\n",
    "        # Transition probabilities P(s' | s, a)\n",
    "        # (Matches the stochastic dynamics used above where applicable.)\n",
    "        self.P = {\n",
    "            \"Class1\": {\n",
    "                \"study\": [(1.0, \"Class2\")],\n",
    "                \"facebook\": [(1.0, \"Facebook\")],\n",
    "            },\n",
    "            \"Class2\": {\n",
    "                \"study\": [(1.0, \"Class3\")],\n",
    "                \"sleep\": [(1.0, \"Sleep\")],\n",
    "            },\n",
    "            \"Class3\": {\n",
    "                \"study\": [(1.0, \"Pass\")],\n",
    "                \"pub\": [(1.0, \"Pub\")],\n",
    "            },\n",
    "            \"Facebook\": {\n",
    "                # Either keep browsing or (eventually) quit;\n",
    "                # we expose an explicit quit action as well.\n",
    "                \"facebook\": [(0.9, \"Facebook\"), (0.1, \"Class1\")],\n",
    "                \"quit\": [(1.0, \"Class1\")],\n",
    "            },\n",
    "            \"Pub\": {\n",
    "                \"drink\": [(0.2, \"Class1\"), (0.4, \"Class2\"), (0.4, \"Class3\")],\n",
    "            },\n",
    "            \"Sleep\": {},\n",
    "            \"Pass\": {},\n",
    "        }\n",
    "\n",
    "        # Reward per action (edit these numbers as desired)\n",
    "        self.R = {\n",
    "            \"study\": -2.0,\n",
    "            \"facebook\": -1.0,\n",
    "            \"sleep\": 0.0,\n",
    "            \"pub\": 1.0,\n",
    "            \"drink\": 1.0,\n",
    "            \"quit\": 0.0,\n",
    "        }\n",
    "\n",
    "    def available_actions(self, state):\n",
    "        return self.actions.get(state, [])\n",
    "\n",
    "    def transitions(self, state, action):\n",
    "        return self.P[state][action]\n",
    "\n",
    "    def reward(self, action):\n",
    "        return float(self.R[action])\n",
    "\n",
    "\n",
    "def value_iteration(mdp: StudentMDP, gamma=0.9, theta=1e-10, max_iters=10_000):\n",
    "    \"\"\"Compute optimal V(s) and greedy policy via value iteration.\n",
    "\n",
    "    Bellman optimality update: V(s) = max_a [ r(a) + gamma * sum_{s'} P(s'|s,a) V(s') ]\n",
    "    Terminal states are fixed to V(s)=0.\n",
    "    \"\"\"\n",
    "    V = {s: 0.0 for s in mdp.states}\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in mdp.states:\n",
    "            if s in mdp.terminal_states:\n",
    "                continue\n",
    "\n",
    "            acts = mdp.available_actions(s)\n",
    "            if not acts:\n",
    "                continue\n",
    "\n",
    "            v_old = V[s]\n",
    "            q_vals = []\n",
    "            for a in acts:\n",
    "                r = mdp.reward(a)\n",
    "                exp_next = sum(p * V[s_next] for p, s_next in mdp.transitions(s, a))\n",
    "                q_vals.append(r + gamma * exp_next)\n",
    "            V[s] = float(max(q_vals))\n",
    "            delta = max(delta, abs(v_old - V[s]))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Greedy policy w.r.t. V\n",
    "    pi = {}\n",
    "    for s in mdp.states:\n",
    "        if s in mdp.terminal_states or not mdp.available_actions(s):\n",
    "            pi[s] = None\n",
    "            continue\n",
    "        best_a = None\n",
    "        best_q = -float(\"inf\")\n",
    "        for a in mdp.available_actions(s):\n",
    "            r = mdp.reward(a)\n",
    "            exp_next = sum(p * V[s_next] for p, s_next in mdp.transitions(s, a))\n",
    "            q = r + gamma * exp_next\n",
    "            if q > best_q:\n",
    "                best_q = q\n",
    "                best_a = a\n",
    "        pi[s] = best_a\n",
    "\n",
    "    return V, pi\n",
    "\n",
    "\n",
    "mdp = StudentMDP()\n",
    "V, pi = value_iteration(mdp, gamma=0.9)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ACTION REWARDS\")\n",
    "print(\"=\" * 70)\n",
    "for a in sorted(mdp.R.keys()):\n",
    "    print(f\"{a:>10s} : {mdp.R[a]:>6.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OPTIMAL VALUE FUNCTION V(s) AND BEST ACTION (pi(s))\")\n",
    "print(\"=\" * 70)\n",
    "for s in mdp.states:\n",
    "    a = pi[s]\n",
    "    a_str = a if a is not None else \"(terminal)\"\n",
    "    print(f\"{s:>8s}  V={V[s]:>8.4f}   pi(s)={a_str}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cop-rl (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
