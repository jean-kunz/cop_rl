{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a92a20",
   "metadata": {},
   "source": [
    "\n",
    "## MDP\n",
    "MDP, or Markov Decision Process : is the main formalism used in Reinforcement Learning\n",
    "\n",
    "\n",
    "**fully observable MDPs** :  the current state that is given to the agent completely characterizes the environment. So the way in which the environment unfolds depends on some state, and we are told that state.”\n",
    "\n",
    "**partially observed** can be converted to MDP and so behave largely the same way\n",
    "\n",
    "Starting with:\n",
    "Markov Chains >> Markov Reward Processes >>  Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3589b88",
   "metadata": {},
   "source": [
    "### Markov chains\n",
    "\n",
    "\n",
    "<img src=\"../image-9.png\" width=\"500px\" />\n",
    "\n",
    "\n",
    "We don’t need to know the full history of states to know what will happen next, just the current one\n",
    "\n",
    "State transition matrix, probability of transition from a state (row) to another one (col):\n",
    "\n",
    "<img src=\"../image-10.png\" width=\"500px\" />\n",
    "\n",
    "\n",
    "A Markov Chain has **no rewards or actions yet**, it simply defines a random process that generates a set of states, each with the Markov property\n",
    "\n",
    "<img src=\"../image-11.png\" width=\"500px\" />\n",
    "\n",
    "\n",
    "Student MDP:\n",
    "\n",
    "<img src=\"../image-12.png\" width=\"500px\" />\n",
    "\n",
    "\n",
    "One can generate random set of states. Ex: ```['C1', 'C2', 'C3', 'Pub', 'C1', 'C2', 'Sleep']````\n",
    "\n",
    "Agent make no decision, only at mercy of chance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b15b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68964bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Class1': [(0.5, 'Class2'), (0.5, 'Facebook')],\n",
       " 'Class2': [(0.8, 'Class3'), (0.2, 'Sleep')],\n",
       " 'Class3': [(0.6, 'Pass'), (0.4, 'Pub')],\n",
       " 'Facebook': [(0.9, 'Facebook'), (0.1, 'Class1')],\n",
       " 'Pub': [(0.2, 'Class1'), (0.4, 'Class2'), (0.4, 'Class3')],\n",
       " 'Sleep': [(1.0, 'Sleep')],\n",
       " 'Pass': [(1.0, 'Pass')]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_map = {\n",
    "    \"C1\": \"Class1\",\n",
    "    \"C2\": \"Class2\",\n",
    "    \"C3\": \"Class3\",\n",
    "    \"FB\": \"Facebook\",\n",
    "    \"Pub\": \"Pub\",\n",
    "    \"Sleep\": \"Sleep\",\n",
    "    \"Pass\": \"Pass\",\n",
    "}\n",
    "\n",
    "states = [\"Class1\", \"Class2\", \"Class3\", \"Facebook\", \"Pub\", \"Sleep\", \"Pass\"]\n",
    "terminal_states = [\"Sleep\", \"Pass\"]\n",
    "\n",
    "transitions = {\n",
    "    \"Class1\": [\n",
    "        (0.5, \"Class2\"),  # Study\n",
    "        (0.5, \"Facebook\"),  # Facebook\n",
    "    ],\n",
    "    \"Class2\": [\n",
    "        (0.8, \"Class3\"),  # Study\n",
    "        (0.2, \"Sleep\"),  # Sleep\n",
    "    ],\n",
    "    \"Class3\": [\n",
    "        (0.6, \"Pass\"),\n",
    "        (0.4, \"Pub\"),\n",
    "    ],\n",
    "    \"Facebook\": [\n",
    "        (0.9, \"Facebook\"),  # Keep browsing\n",
    "        (0.1, \"Class1\"),  # Quit\n",
    "    ],\n",
    "    \"Pub\": [\n",
    "        (0.2, \"Class1\"),  # Leave pub, go to C1 (0.5 * 0.2)\n",
    "        (0.4, \"Class2\"),  # Leave pub, go to C2 (0.5 * 0.4)\n",
    "        (0.4, \"Class3\"),  # Leave pub, go to C3 (0.5 * 0.4)\n",
    "    ],\n",
    "    \"Pass\": [],  # Terminal state\n",
    "    \"Sleep\": [],  # Terminal state\n",
    "}\n",
    "\n",
    "\n",
    "P = {s: [] for s in states}\n",
    "for s, trans in transitions.items():\n",
    "    # s = state_map[s_short]\n",
    "    P[s] = [(p, s_next) for p, s_next in trans]\n",
    "\n",
    "for t in terminal_states:\n",
    "    P[t] = [(1.0, t)]  # we stay at terminal states\n",
    "\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4b9893",
   "metadata": {},
   "source": [
    "### Markov Reward Processes\n",
    "A Markov reward process is exactly like a Markov chain, except each step taken generates a reward.\n",
    "\n",
    "<img src=\"../image-13.png\" width=\"500px\" />\n",
    "<div/>\n",
    "\n",
    "<img src=\"../image-14.png\" width=\"500px\" />\n",
    "\n",
    "\n",
    "trajectories we sample will have an associated return value.\n",
    "\n",
    "<img src=\"../image-15.png\" width=\"500px\" />\n",
    "\n",
    "\n",
    "Gamma (0..1) determines how important future rewards are to the agent. if close to 0, consider immediate reward only.\n",
    "\n",
    "Gamma=1 (for the rest of course for convenience)\n",
    "\n",
    "Value function: expectation over future rewards \n",
    "\n",
    "\n",
    "<img src=\"../image-16.png\" width=\"500px\" />\n",
    "\n",
    "\n",
    "Thanks to the law of large numbers, a rough estimate of the true value function for a state can be achieved by sampling many trajectories starting from that state, and averaging the sampled returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a8db82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Class1': -2.0,\n",
       " 'Class2': -2.0,\n",
       " 'Class3': -2.0,\n",
       " 'Facebook': -1.0,\n",
       " 'Pub': 1.0,\n",
       " 'Sleep': 0.0,\n",
       " 'Pass': 10.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = {\n",
    "    \"Class1\": -2.0,\n",
    "    \"Class2\": -2.0,\n",
    "    \"Class3\": -2.0,\n",
    "    \"Facebook\": -1.0,\n",
    "    \"Pub\": 1.0,\n",
    "    \"Sleep\": 0.0,\n",
    "    \"Pass\": 10.0,\n",
    "}\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "187d3bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transitions(state):\n",
    "    \"\"\"Transition function for the MDP\"\"\"\n",
    "    return P[state]\n",
    "\n",
    "\n",
    "def reward(state) -> float:\n",
    "    \"\"\"Reward function for the MDP\"\"\"\n",
    "    return float(r[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44d7aa88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Facebook'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step(state):\n",
    "    \"\"\"Take a step in the Markov Chain from current state\"\"\"\n",
    "    if state in terminal_states:\n",
    "        return state\n",
    "\n",
    "    if state not in transitions:\n",
    "        raise ValueError(f\"Invalid state: {state}\")\n",
    "\n",
    "    # Get possible transitions\n",
    "    possible_transitions = transitions[state]\n",
    "\n",
    "    if not possible_transitions:\n",
    "        return state\n",
    "\n",
    "    # Sample next state based on probabilities\n",
    "    probs = [t[0] for t in possible_transitions]\n",
    "    idx = np.random.choice(len(possible_transitions), p=probs)\n",
    "    _, next_state = possible_transitions[idx]\n",
    "\n",
    "    return next_state\n",
    "\n",
    "\n",
    "step(\"Class1\")  # Example step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fb20496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Class1': -12.543209876532266,\n",
       " 'Class2': 1.4567901234579068,\n",
       " 'Class3': 4.320987654322317,\n",
       " 'Facebook': -22.543209876523473,\n",
       " 'Pub': 0.8024691358056364,\n",
       " 'Sleep': 0.0,\n",
       " 'Pass': 10.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 1.0\n",
    "theta = 1e-12\n",
    "max_iters = 30_000\n",
    "\n",
    "V = {s: 0.0 for s in states}\n",
    "\n",
    "# Terminal states yield their reward once and then the episode ends.\n",
    "# So we set their values directly, and do not update them in the loop.\n",
    "for t in terminal_states:\n",
    "    V[t] = reward(t)\n",
    "\n",
    "for _ in range(max_iters):\n",
    "    delta = 0.0\n",
    "\n",
    "    for s in states:\n",
    "        if s in terminal_states:\n",
    "            continue\n",
    "        v_old = V[s]\n",
    "        exp_next = sum(p * V[s_next] for p, s_next in get_transitions(s))\n",
    "        V[s] = reward(s) + gamma * exp_next\n",
    "        delta = max(delta, abs(v_old - V[s]))\n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1370aa9",
   "metadata": {},
   "source": [
    "#### Bellman equation\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../image-17.png\" width=\"500px\" />\n",
    "\n",
    "\n",
    "\n",
    "as gamma is constant:\n",
    "\n",
    "\n",
    "<img src=\"../image-18.png\" width=\"500px\" />\n",
    "<div/>\n",
    "\n",
    "<img src=\"../image-19.png\" width=\"500px\" />\n",
    "<div/>\n",
    "\n",
    "<img src=\"../image-20.png\" width=\"500px\" />\n",
    "\n",
    "\n",
    "So : **value of a state s is the expectation over the immediate reward plus the discounted expected future reward***\n",
    "\n",
    "\n",
    "##### How do we get that expected reward of the next state v(St+1)?\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../image-21.png\" width=\"500px\" />\n",
    "\n",
    "\n",
    "= sum of all state-values weighted by their probability of occurring. Many of the values in this sum will be zero, because you can only transition to some states at each step\n",
    "\n",
    "Notice that the immediate reward is no longer expressed as an expectation because it’s a constant, and the expectation of a constant is just a constant, so we can pull it out of the expectation.\n",
    "\n",
    "**Bellman equation** : expresses the state-value in terms of itself .\n",
    "\n",
    "<img src=\"../image-22.png\" width=\"500px\" />\n",
    "\n",
    "Nodes b1 and b2 are possible next states reachable from node a\n",
    "\n",
    "\n",
    "\n",
    "we’re taking the immediate reward r and then averaging over the values of all possible successor states\n",
    "\n",
    "Self check with former value calculated iteratively:\n",
    "\n",
    "<img src=\"../image-23.png\" width=\"500px\" />\n",
    "\n",
    "Different way to solve it:\n",
    "\n",
    "##### Analytical solution\n",
    "\n",
    "\n",
    "<img src=\"../image-24.png\" width=\"500px\" />\n",
    "<div/>\n",
    "\n",
    "<img src=\"../image-25.png\" width=\"500px\" />\n",
    "\n",
    "<div/>\n",
    "<img src=\"../image-26.png\" width=\"500px\" />\n",
    "\n",
    "Often to large to be used\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d7c7a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class1 -12.543209876543214\n",
      "Class2 1.4567901234567908\n",
      "Class3 4.320987654320986\n",
      "Facebook 10.0\n",
      "Pub 0.8024691358024674\n",
      "Sleep -22.543209876543223\n",
      "Pass 0.0\n"
     ]
    }
   ],
   "source": [
    "## Analytical Value Calculation with bellman Equations\n",
    "\n",
    "\n",
    "_rewards = [-2, -2, -2, 10, 1, -1, 0]\n",
    "p_matrix = [\n",
    "    [0, 0.5, 0, 0, 0, 0.5, 0],\n",
    "    [0, 0, 0.8, 0, 0, 0, 0.2],\n",
    "    [0, 0, 0, 0.6, 0.4, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1],\n",
    "    [0.2, 0.4, 0.4, 0, 0, 0, 0],\n",
    "    [0.1, 0, 0, 0, 0, 0.9, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "]\n",
    "gamma = 1\n",
    "R = np.array(_rewards)\n",
    "P = np.matrix(p_matrix)\n",
    "I = np.identity(len(p_matrix))\n",
    "solution = np.dot(np.linalg.inv((I - gamma * P)), R)\n",
    "solution = solution.tolist()[0]\n",
    "for state in range(len(states)):\n",
    "    print(states[state], solution[state])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac76987",
   "metadata": {},
   "source": [
    "##### Iterative solutions\n",
    "\n",
    "Do it later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aac5268",
   "metadata": {},
   "source": [
    "### Markov Decision Processes (MDP)\n",
    "\n",
    "It introduces actions. The inclusion of actions means that the transition probabilities as well as the rewards must now be **conditioned on a chosen action**\n",
    "\n",
    "\n",
    "<img src=\"../image-27.png\" width=\"500px\" /><div/>\n",
    "\n",
    "\n",
    "There is now a separate state transition matrix and reward for every action\n",
    "\n",
    "<img src=\"../image-28.png\" width=\"500px\" /><div/>\n",
    "\n",
    "For this particular MDP, every action save for “Pub” is deterministic. In the case of the action “ Pub”, the agent no longer enters into the “Pub” state, but instead transitions with some probability into one of the 3 “Class” states.\n",
    "\n",
    "Why reduce the “Pub” state to an action instead? To show that actions can have both deterministic and stochastic effects on where the agent ends up in the environment.\n",
    "\n",
    "we also don’t need the “Pass” state anymore. Whereas in the MC and MRP examples we only got rewards for exiting states, we now get rewards after taking actions, so we can get the +10 reward for studying in Class 3 without needing to transition into a “Pass” state that immediately takes us to “Sleep”\n",
    "\n",
    "#### Policies\n",
    "\n",
    "Agent policy:  a function that tells the agent what action to take depending on the state (or a probability distribution to sample from). Policy can also be deterministic (no sampling)\n",
    "\n",
    "\n",
    "<img src=\"../image-29.png\" width=\"500px\" /><div/>\n",
    "\n",
    "Later: Choosing a policy impacts value function. Calculate value function under many policies and select policy with better value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25155d7",
   "metadata": {},
   "source": [
    "##### Any MDP to MRP\n",
    "\n",
    "We can reformulate any MDP into a MRP (which we know how to solve analytically). Change MDP to be action independent. \n",
    "\n",
    "The rewards in the MDP are conditioned on which action is taken, so we need to average over all actions from a single state to get a single action-independent reward value. Each state has an expected reward for any action\n",
    "\n",
    "<img src=\"../image-30.png\" width=\"300px\" /><div/>\n",
    "<img src=\"../image-31.png\" width=\"300px\" /><div/>\n",
    "<img src=\"../image-32.png\" width=\"500px\" /><div/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46d02094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1 -1.307692307692308\n",
      "C2 2.6923076923076925\n",
      "C3 7.384615384615385\n",
      "FB -2.3076923076923075\n",
      "Sleep 0.0\n"
     ]
    }
   ],
   "source": [
    "# Transform MDP to a MRP\n",
    "\n",
    "# Probabilities changed to reflect uniform random policy\n",
    "# Notice Class 3 probabilities reflect possible \"Pub\" choice:\n",
    "# (.5 * .2) = probability of picking pub (.5) AND\n",
    "# probability of then being sent to class 1 (.2)\n",
    "# Together they mean a total probability of (.5 * .2) = .1\n",
    "# for ending up back in C1 from C3\n",
    "\n",
    "state_names = [\"C1\", \"C2\", \"C3\", \"FB\", \"Sleep\"]\n",
    "\n",
    "p_matrix = [\n",
    "    [0, 0.5, 0, 0.5, 0],\n",
    "    [0, 0, 0.5, 0, 0.5],\n",
    "    [0.1, 0.2, 0.2, 0, 0.5],\n",
    "    [0.5, 0, 0, 0.5, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "]\n",
    "# Action rewards are also weighted and summed by probability of occurring\n",
    "# I.E: 5.5 = (.5 * 10) + (.5 * 1)\n",
    "_rewards = [-1.5, -1, 5.5, -0.5, 0]\n",
    "gamma = 1\n",
    "R = np.array(_rewards)\n",
    "P = np.matrix(p_matrix)\n",
    "I = np.identity(len(p_matrix))\n",
    "solution = np.dot(np.linalg.inv((I - gamma * P)), R)\n",
    "solution = solution.tolist()[0]\n",
    "for state in range(len(state_names)):\n",
    "    print(state_names[state], solution[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a41310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b86b8ba4",
   "metadata": {},
   "source": [
    "\n",
    "#### MDP Value Functions\n",
    "\n",
    "<img src=\"../image-33.png\" width=\"500px\" /><div/>\n",
    "\n",
    "The state-value function tells us how much reward we can expect from the current state onward, if we follow the chosen policy.\n",
    "\n",
    "<img src=\"../image-34.png\" width=\"500px\" /><div/>\n",
    "\n",
    "we can use an average over many samples to estimate the action-value function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71dfd3",
   "metadata": {},
   "source": [
    "#### MDP Bellman Equation\n",
    "\n",
    "<img src=\"../image-35.png\" width=\"500px\" /><div/>\n",
    "<img src=\"../image-36.png\" width=\"500px\" /><div/>\n",
    "\n",
    "\n",
    "\n",
    "They represent the same underlying value: the expected future reward\n",
    "\n",
    "<img src=\"../image-37.png\" width=\"500px\" /><div/>\n",
    "\n",
    "\n",
    "**States** = Open Circles; **Actions** = Closed Circles\n",
    "\n",
    "State-value function is the average of all the available action-values, weighted by how likely we are to choose them under our current policy:\n",
    "\n",
    "<img src=\"../image-38.png\" width=\"500px\" /><div/>\n",
    "\n",
    "Similarly, the action-value function is the average value of the states the agent might transition to given the chosen action, but weighted instead by their transition probabilities\n",
    "\n",
    "<img src=\"../image-39.png\" width=\"500px\" /><div/>\n",
    "\n",
    "<img src=\"../image-40.png\" width=\"500px\" /><div/>\n",
    "\n",
    "\n",
    "S=possible next states, P=transition probabilities\n",
    "\n",
    "So action-value is the immediate reward for taking that action, plus the expected value of the next state, over all possible successor states\n",
    "\n",
    "-----\n",
    "\n",
    "Expand diagram to 2 steps to get MDP bellmann equation\n",
    "\n",
    "<img src=\"../image-41.png\" width=\"500px\" /><div/>\n",
    "\n",
    "<img src=\"../image-42.png\" width=\"500px\" /><div/>\n",
    "\n",
    "---- \n",
    "also start from an action node:\n",
    "\n",
    "<img src=\"../image-43.png\" width=\"500px\" /><div/>\n",
    "\n",
    "\n",
    "And expand the action-value function by substituting in the state-value function:\n",
    "\n",
    "<img src=\"../image-44.png\" width=\"500px\" /><div/>\n",
    "-----\n",
    "SO we get the **value function in terms of itself**, giving us the **MDP Bellman Equation**.\n",
    "\n",
    "\n",
    "<img src=\"../image-45.png\" width=\"500px\" /><div/>\n",
    "\n",
    "It implements this formula:\n",
    "\n",
    "<img src=\"../image-46.png\" width=\"500px\" /><div/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0edad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V^pi (uniform random):\n",
      "     C1: -1.307692\n",
      "     C2:  2.692308\n",
      "     C3:  7.384615\n",
      "     FB: -2.307692\n",
      "  Sleep:  0.000000\n",
      "\n",
      "V* (optimal):\n",
      "     C1:  6.000000\n",
      "     C2:  8.000000\n",
      "     C3:  10.000000\n",
      "     FB:  6.000000\n",
      "  Sleep:  0.000000\n",
      "\n",
      "Greedy optimal policy π*:\n",
      "     C1 -> study\n",
      "     C2 -> study\n",
      "     C3 -> study\n",
      "     FB -> quit\n",
      "\n",
      "Q* (optimal action-values):\n",
      "     C1: study=6.000000, facebook=5.000000\n",
      "     C2: study=8.000000, sleep=0.000000\n",
      "     C3: study=10.000000, pub=9.400000\n",
      "     FB: quit=6.000000, facebook=5.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---------- MDP definition (dicts) ----------\n",
    "\n",
    "S = list(state_names)  # [\"C1\", \"C2\", \"C3\", \"FB\", \"Sleep\"]\n",
    "terminal_states_mdp = {\"Sleep\"}\n",
    "\n",
    "A = {\n",
    "    \"C1\": [\"study\", \"facebook\"],\n",
    "    \"C2\": [\"study\", \"sleep\"],\n",
    "    \"C3\": [\"study\", \"pub\"],\n",
    "    \"FB\": [\"quit\", \"facebook\"],\n",
    "    \"Sleep\": [],\n",
    "}\n",
    "\n",
    "# Transition model: P[(s,a)] = [(prob, s_next), ...]\n",
    "P_sa = {\n",
    "    (\"C1\", \"study\"): [(1.0, \"C2\")],\n",
    "    (\"C1\", \"facebook\"): [(1.0, \"FB\")],\n",
    "    (\"C2\", \"study\"): [(1.0, \"C3\")],\n",
    "    (\"C2\", \"sleep\"): [(1.0, \"Sleep\")],\n",
    "    # Study in C3 gives +10 then day ends\n",
    "    (\"C3\", \"study\"): [(1.0, \"Sleep\")],\n",
    "    # Pub in C3 transitions back to class states\n",
    "    (\"C3\", \"pub\"): [(0.2, \"C1\"), (0.4, \"C2\"), (0.4, \"C3\")],\n",
    "    (\"FB\", \"quit\"): [(1.0, \"C1\")],\n",
    "    (\"FB\", \"facebook\"): [(1.0, \"FB\")],\n",
    "}\n",
    "\n",
    "# Reward model: R[(s,a)] = immediate reward for taking action a in s\n",
    "R_sa = {\n",
    "    (\"C1\", \"study\"): -2.0,\n",
    "    (\"C1\", \"facebook\"): -1.0,\n",
    "    (\"C2\", \"study\"): -2.0,\n",
    "    (\"C2\", \"sleep\"): 0.0,\n",
    "    (\"C3\", \"study\"): 10.0,\n",
    "    (\"C3\", \"pub\"): 1.0,\n",
    "    (\"FB\", \"quit\"): 0.0,\n",
    "    (\"FB\", \"facebook\"): -1.0,\n",
    "}\n",
    "\n",
    "\n",
    "def actions(s):\n",
    "    return A[s]\n",
    "\n",
    "\n",
    "def transitions_sa(s, a):\n",
    "    return P_sa[(s, a)]\n",
    "\n",
    "\n",
    "def reward_sa(s, a):\n",
    "    return float(R_sa[(s, a)])\n",
    "\n",
    "\n",
    "# ---------- Bellman operators ----------\n",
    "\n",
    "\n",
    "def bellman_expectation_backup_v(V, pi, gamma):\n",
    "    \"\"\"Bellman expectation equation for V under policy pi.\n",
    "\n",
    "    V(s) = Σ_a π(a|s) Σ_{s'} P(s'|s,a) [ r(s,a) + γ V(s') ]\n",
    "    \"\"\"\n",
    "    V_new = {}\n",
    "    for s in S:\n",
    "        if s in terminal_states_mdp:\n",
    "            V_new[s] = 0.0\n",
    "            continue\n",
    "        v = 0.0\n",
    "        for a, p_a in pi[s].items():\n",
    "            q_sa = 0.0\n",
    "            for p, s_next in transitions_sa(s, a):\n",
    "                q_sa += p * (reward_sa(s, a) + gamma * V[s_next])\n",
    "            v += p_a * q_sa\n",
    "        V_new[s] = v\n",
    "    return V_new\n",
    "\n",
    "\n",
    "def q_from_v(V, gamma):\n",
    "    \"\"\"Compute Q(s,a) from V: Q(s,a) = Σ_{s'} P(s'|s,a) [ r(s,a) + γ V(s') ].\"\"\"\n",
    "    Q = {s: {} for s in S}\n",
    "    for s in S:\n",
    "        if s in terminal_states_mdp:\n",
    "            continue\n",
    "        for a in actions(s):\n",
    "            q = 0.0\n",
    "            for p, s_next in transitions_sa(s, a):\n",
    "                q += p * (reward_sa(s, a) + gamma * V[s_next])\n",
    "            Q[s][a] = q\n",
    "    return Q\n",
    "\n",
    "\n",
    "# ---------- Dynamic programming algorithms ----------\n",
    "\n",
    "\n",
    "def normalize_policy(pi):\n",
    "    \"\"\"Ensure pi[s] is a valid distribution over actions(s).\"\"\"\n",
    "    out = {}\n",
    "    for s in S:\n",
    "        if s in terminal_states_mdp:\n",
    "            out[s] = {}\n",
    "            continue\n",
    "        if s not in pi or not pi[s]:\n",
    "            # default uniform\n",
    "            acts = actions(s)\n",
    "            out[s] = {a: 1.0 / len(acts) for a in acts}\n",
    "            continue\n",
    "\n",
    "        # keep only valid actions\n",
    "        probs = {a: float(p) for a, p in pi[s].items() if a in set(actions(s))}\n",
    "        total = sum(probs.values())\n",
    "        if total <= 0:\n",
    "            acts = actions(s)\n",
    "            out[s] = {a: 1.0 / len(acts) for a in acts}\n",
    "        else:\n",
    "            out[s] = {a: p / total for a, p in probs.items()}\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42899a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V^pi (uniform random):\n",
      "     C1: -1.307692\n",
      "     C2:  2.692308\n",
      "     C3:  7.384615\n",
      "     FB: -2.307692\n",
      "  Sleep:  0.000000\n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation(pi, gamma=1.0, theta=1e-12, max_iters=100_000):\n",
    "    \"\"\"Iterative policy evaluation using the Bellman expectation backup.\"\"\"\n",
    "    pi = normalize_policy(pi)\n",
    "    V = {s: 0.0 for s in S}\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        V_new = bellman_expectation_backup_v(V, pi, gamma)\n",
    "        delta = max(abs(V_new[s] - V[s]) for s in S)\n",
    "        V = V_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    Q = q_from_v(V, gamma)\n",
    "    return V, Q\n",
    "\n",
    "\n",
    "# Uniform random policy (matches the earlier MDP->MRP conversion)\n",
    "pi_uniform = {s: {a: 1.0 / len(actions(s)) for a in actions(s)} for s in S}\n",
    "\n",
    "V_pi, Q_pi = policy_evaluation(pi_uniform, gamma=1.0, theta=1e-12)\n",
    "print(\"V^pi (uniform random):\")\n",
    "for s in S:\n",
    "    print(f\"  {s:>5}: {V_pi[s]: .6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9928f856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "V* (optimal):\n",
      "     C1:  6.000000\n",
      "     C2:  8.000000\n",
      "     C3:  10.000000\n",
      "     FB:  6.000000\n",
      "  Sleep:  0.000000\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(gamma=1.0, theta=1e-12, max_iters=100_000):\n",
    "    \"\"\"Value iteration using the Bellman optimality backup.\n",
    "\n",
    "    V(s) = max_a Σ_{s'} P(s'|s,a)[ r(s,a) + γ V(s') ]\n",
    "    \"\"\"\n",
    "    V = {s: 0.0 for s in S}\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        V_new = dict(V)\n",
    "        for s in S:\n",
    "            if s in terminal_states_mdp:\n",
    "                V_new[s] = 0.0\n",
    "                continue\n",
    "            best = -float(\"inf\")\n",
    "            for a in actions(s):\n",
    "                q = 0.0\n",
    "                for p, s_next in transitions_sa(s, a):\n",
    "                    q += p * (reward_sa(s, a) + gamma * V[s_next])\n",
    "                best = max(best, q)\n",
    "            delta = max(delta, abs(best - V[s]))\n",
    "            V_new[s] = best\n",
    "        V = V_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    Q = q_from_v(V, gamma)\n",
    "\n",
    "    # Greedy deterministic policy from Q*\n",
    "    pi_star = {}\n",
    "    for s in S:\n",
    "        if s in terminal_states_mdp:\n",
    "            pi_star[s] = {}\n",
    "            continue\n",
    "        best_a = max(Q[s].items(), key=lambda kv: kv[1])[0]\n",
    "        pi_star[s] = {a: (1.0 if a == best_a else 0.0) for a in actions(s)}\n",
    "\n",
    "    return V, Q, pi_star\n",
    "\n",
    "\n",
    "# ---------- Run on the same example ----------\n",
    "\n",
    "V_star, Q_star, pi_star = value_iteration(gamma=1.0, theta=1e-12)\n",
    "print(\"\\nV* (optimal):\")\n",
    "for s in S:\n",
    "    print(f\"  {s:>5}: {V_star[s]: .6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fda1a54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Greedy optimal policy π*:\n",
      "     C1 -> study\n",
      "     C2 -> study\n",
      "     C3 -> study\n",
      "     FB -> quit\n",
      "\n",
      "Q* (optimal action-values):\n",
      "     C1: study=6.000000, facebook=5.000000\n",
      "     C2: study=8.000000, sleep=0.000000\n",
      "     C3: study=10.000000, pub=9.400000\n",
      "     FB: quit=6.000000, facebook=5.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGreedy optimal policy π*:\")\n",
    "for s in S:\n",
    "    if s in terminal_states_mdp:\n",
    "        continue\n",
    "    best_a = max(pi_star[s].items(), key=lambda kv: kv[1])[0]\n",
    "    print(f\"  {s:>5} -> {best_a}\")\n",
    "\n",
    "print(\"\\nQ* (optimal action-values):\")\n",
    "for s in S:\n",
    "    if s in terminal_states_mdp:\n",
    "        continue\n",
    "    items = \", \".join(f\"{a}={Q_star[s][a]:.6f}\" for a in actions(s))\n",
    "    print(f\"  {s:>5}: {items}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d0a58c",
   "metadata": {},
   "source": [
    "\n",
    "### Optimal Value Functions\n",
    "\n",
    "<img src=\"../image-47.png\" width=\"500px\" /><div/>\n",
    "\n",
    "<img src=\"../image-48.png\" width=\"500px\" /><div/>\n",
    "\n",
    "\n",
    "<img src=\"../image-49.png\" width=\"500px\" /><div/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831262d",
   "metadata": {},
   "source": [
    "### Optimal policies\n",
    "\n",
    "<img src=\"../image-50.png\" width=\"500px\" /><div/>\n",
    "\n",
    "<img src=\"../image-51.png\" width=\"500px\" /><div/>\n",
    "\n",
    "<img src=\"../image-52.png\" width=\"500px\" /><div/>\n",
    "\n",
    "\n",
    "<img src=\"../image-53.png\" width=\"500px\" /><div/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6379d3",
   "metadata": {},
   "source": [
    "#### Finding The Optimal Action-Value Function\n",
    "\n",
    "<img src=\"../image-54.png\" width=\"500px\" /><div/>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../image-56.png\" width=\"500px\" /><div/>\n",
    "\n",
    "<img src=\"../image-56.png\" width=\"500px\" /><div/>\n",
    "\n",
    "<img src=\"../image-57.png\" width=\"500px\" /><div/>\n",
    "\n",
    "<img src=\"../image-58.png\" width=\"500px\" /><div/>\n",
    "\n",
    "<img src=\"../image-59.png\" width=\"500px\" /><div/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487c07ea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cop-rl (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
