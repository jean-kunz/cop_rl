{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07a82630",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "\n",
    "## Sources\n",
    "- https://medium.com/@lgendrot/teaching-myself-reinforcement-learning-7b4157ee3b68\n",
    "- https://medium.com/@sriramp_98201/reinforcement-learning-from-intuition-to-actor-critic-and-ppo-db3d1e3907a1\n",
    "- https://medium.com/@reddyyashu20/reinforcement-learning-tutorial-theo-97a6caa5d5f7\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b5303",
   "metadata": {},
   "source": [
    "Reinforcement Learning is about learning behavior through interaction and experiences NOT with labeled data.\n",
    "\n",
    "An agent repeatedly:\n",
    "\n",
    "- Observes the current state s\n",
    "- Chooses an action a\n",
    "- Receives a reward r\n",
    "- Moves to a new state s′\n",
    "\n",
    "The agent’s goal is simple to state, but hard to solve - Maximize total future reward, not just immediate reward. \n",
    "\n",
    "It learns which action lead to positive feedback or rewards and which one to negative feedback. \n",
    "\n",
    "<img src=\"../image-61.png\" width=\"500px\"/>\n",
    "\n",
    "\n",
    "<img src=\"../image.png\" width=\"500px\"/>\n",
    "\n",
    "## Observations\n",
    "Agents need to get information about the environment in order to make decisions on how to act. In the Atari example the observations might amount to screenshots of the game being played, the same as a human would see\n",
    "\n",
    "## Agents\n",
    "The agent receives observations of the environment, and uses those observations to select actions, which affects the environment and ultimately causes a reward signal to be given to the agent.\n",
    "\n",
    "\n",
    "## States and the Markov Property\n",
    "In the course of operation, the agent generates a “path” through an environment, which consists of sequential sets of observations, rewards, and next actions for each step the agent takes.\n",
    "\n",
    "The history of an agent/environment pair includes all of the observations, rewards, and actions generated up to a particular point in time. \n",
    "\n",
    "<img src=\"../image-2.png\" width=\"500px\"/>\n",
    "\n",
    "\n",
    "It’s unnecessary to store the entire history of an agent -> concept of state. The state is formally just a function that produces a summary of the history up to a certain point.\n",
    "\n",
    "<img src=\"../image-3.png\" width=\"200px\" />\n",
    "\n",
    "\n",
    "The state can be thought of from both the perspective of the environment (environment state) and the agent (agent state, built entirely by the developer and by the algorithm used to solve the reinforcement learning problem.).\n",
    "\n",
    "Ideally any state, being a summary of the history, would contain all the useful information about the history\n",
    "\n",
    "<img src=\"../image-1.png\" width=\"500px\" />\n",
    "\n",
    "\n",
    "\n",
    "**fully observable** environment, the agent state is the same as the environment state.\n",
    "\n",
    "**partially observable** environment the agent state must be somehow computed from the history, since the observations are now incomplete representations of the environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a43ec",
   "metadata": {},
   "source": [
    "### Return and long term thinking\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "- $\\gamma$ : discount factor. Close to 1 -> long term planning. Close to 0 -> short term planning\n",
    "\n",
    "### Value function: compressing the future\n",
    "\n",
    "<img src=\"../image-6.png\" width=\"500px\" />\n",
    "\n",
    "Estimate of how much future reward the agent can expect to receive if it is governed by our current policy.\n",
    "\n",
    "Expected return when starting from state s -> how good to be in one state.\n",
    "\n",
    "Parameterized by a **discount** factor (gamma in the above formula)\n",
    "\n",
    "### Q-value:\n",
    "\n",
    "Similar to value but parametrized by an action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa035405",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Anatomy of a Reinforcement Learning Agent\n",
    "\n",
    "<img src=\"../image-4.png\" width=\"500px\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5eae6b",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "“What’s going to happen next if I do this?”\n",
    "\n",
    "Agent’s learned conception of how the environment works. Given a particular state and an action to take, the agent’s model will give it a prediction of the next state\n",
    "\n",
    "Can also predict the reward given a state and action pair.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../image-7.png\" width=\"500px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5447c6df",
   "metadata": {},
   "source": [
    "## Categories of agents\n",
    "\n",
    "\n",
    "<img src=\"../image-8.png\" width=\"500px\" />\n",
    "\n",
    "\n",
    "### Model free\n",
    "\n",
    "it learns policy directly from experience with environment. No model of environment dynamics\n",
    "\n",
    "#### value based\n",
    "\n",
    " where it has no explicit policy, but instead learns a value function and acts based on its estimates. It finds the optimal value function -> max value at a state(Technically it still has an implicit policy: choose the best action).\n",
    "\n",
    "\n",
    "#### policy based\n",
    "\n",
    "learning a policy directly without learning a value function. The policy it learns apply the action to maximize future reward.\n",
    "\n",
    "- **deterministic**: same action is produced by policy at a given state, $a=\\pi(s)$\n",
    "\n",
    "- **stochastic**: output a probability distribution over actions again a given state (which are sampled), $\\pi(a|s)=P[A_t=a|S_t=s]$\n",
    "\n",
    "\n",
    "#### actor/critic\n",
    "\n",
    "A mix of value and policy based, having both a learned policy and an estimated value function for that learned policy.\n",
    "\n",
    "\n",
    "### Model-based\n",
    "A model of the environment is created and the agent explores it to learn it. Model is different for every env.\n",
    "\n",
    "\n",
    "Use the model for planning to derive a policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7e09ca",
   "metadata": {},
   "source": [
    "## Planning vs control\n",
    "\n",
    "### Planning\n",
    "\n",
    "Thinking what to do\n",
    "\n",
    "Use model to simulate future trajectories and optimize over them\n",
    "\n",
    "### Control\n",
    "\n",
    "Execute actions (do it)\n",
    "\n",
    "A control problem determine the best possible action to take to maximize long term reward \n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "Control problem is optimization that tries to find optimal policy.\n",
    "\n",
    "Non control problem (like iterative policy iteration) is about predicting a value of a state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2764cd91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a03c3a0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Exploration vs Exploitation\n",
    "\n",
    "- Exploitation: Use current knowledge to maximize reward\n",
    "- Exploration: Try new things to improve knowledge\n",
    "\n",
    "Stochastic policies have built-in exploration (sampling using temperature params)\n",
    "\n",
    "Deterministic policies need explicit exploration mechanism like **e-greedy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27940a74",
   "metadata": {},
   "source": [
    "## Model Map\n",
    "\n",
    "RL Hierarchy:\n",
    "\n",
    "**Dynamic programming (DP)**: model-based, assume environment is known (state transition probabilities are known). \n",
    "\n",
    "Use of bellman equation in \n",
    "- policy iteration \n",
    "- value iteration\n",
    "\n",
    "**Model-free RL**: learns from sample. Environment is not known. It learns it by estimating action values\n",
    "- Temporal difference:\n",
    "    - Q-Learning(Off-Policy): it update its knowledge by comparing what its predicted reward and the real reward. It always think he taking the best action (max)\n",
    "        - Deep Q-Network, q-learning with neural network as used by deep mind in atari games. \n",
    "    - SARSA (On-Policy): use real action value  (more cautious approach)\n",
    "- Monte carlo methods\n",
    "- Policy gradients: REINFORCE, PPO (actor critic)\n",
    "\n",
    "**Model-based RL**: learns the model, then uses it with DP-like planning\n",
    "- Dyna-Q, AlphaZero, MuZero, world models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ecc21",
   "metadata": {},
   "source": [
    "<img src=\"../image-60.png\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2bfc1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c51c552a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
